---
layout: default
title: Federated Learning
parent: 7. 학습 기법
grand_parent: AI (인공지능)
nav_order: 6
---

# 연합학습(Federated Learning)
{: .fs-8 }

학습 기법
{: .label .label-green }

---

## 핵심 키워드

`전역모델` `지역모델` `FedSGD` `FedAVG`

---

## 정의/개념

저장 데이터를 직접 공유하지 않는 다수의 로컬 기기와 하나의 중앙 서버가 협력하여 AI 모델을 학습하는 분산형 머신 러닝

---

## 동작원리

```
                          ① broadcast              ④ average
        Global    ◆─────────────────────▶◆◀───────────────────◆
                                    
        ──────────────────────────────────────────────────────────────
        
        Local            ② local update
                    ┌─────────────┐   ┌─────────────┐   ┌─────────────┐
                    │    ◆───▶◆   │   │    ◆───▶◆   │   │    ◆───▶◆   │
                    │             │   │             │   │             │
                    │    ◆───▶◆   │   │    ◆───▶◆   │   │    ◆───▶◆   │
                    └──────┬──────┘   └──────┬──────┘   └──────┬──────┘
                           │                 │                 │
                           └─────────────────┴─────────────────┘
                                             │
                                      ③ aggregate
```

---

## 프로세스

| 절차 | 설명 |
|:-----|:-----|
| **① 전역(Global) 모델 분배 (Broadcast)** | 서버는 사전에 정의한 최적 참여자를 선정한 후 각 단말로 수행해야 할 작업 관련 정보를 전달 |
| **② 지역 모델 갱신 (Local Update)** | 단말에 저장된 개인 데이터를 사용하여 로컬 AI 모델을 생성 |
| **③ 지역 모델 취합 (Aggregate)** | 서버와 접속 등 특정 조건 만족 시, 단말은 생성한 로컬 AI 모델의 결과값(파라미터)을 압축·암호화하여 서버로 전달 |
| **④ 전역 모델 갱신 (Global Update)** | 취합된 값을 이용하여 전역 모델을 갱신 |

---

## 알고리즘

```
        ┌─────────┐         x₁                  if C = 1,  x_new = (x₁+x₂···+xₙ)/n
        │  단말 1  │─────────────────┐
        └─────────┘                  │                    x_old = x_new
        ┌─────────┐         x₂      │
        │  단말 2  │─────────────────┼─────────▶┌──────┐
        └─────────┘                  │          │ 서버  │
             :               :       │          └──────┘
        ┌─────────┐         xₙ      │
        │  단말 n  │─────────────────┘     x_old = x_new
        └─────────┘
                                        ◀───────────────▶
                                            1 Epoch
        
        FedSGD: 1회 학습        Batch size       데이터셋        FedAVG: K회 학습
```

| 알고리즘 | 설명 |
|:---------|:-----|
| **FedSGD (Federated Stochastic Gradient Descent)** | 각 단말에서 한번 학습한 파라미터를 중앙 서버로 전달<br>- 중앙 서버는 취합한 로컬 파라미터 평균 계산 후 글로벌 파라미터 갱신<br>- 갱신된 글로벌 파라미터가 수렴 조건 만족 시까지 과정 반복 |
| **FedAVG (Federated Averaging)** | 각 단말에서 일정 횟수 K만큼 반복 수행 후 파라미터를 서버로 전달<br>- Batch Size 크기로 분할 학습하여 minibatch 효과를 주어 글로벌 파라미터가 수렴에 이르는 시간을 단축 |

---

## 장점과 활용

| 장점 | 설명 |
|:-----|:-----|
| **프라이버시 보호** | 원본 데이터 서버 전송 없음 |
| **통신 비용 절감** | 모델 파라미터만 전송 |
| **분산 처리** | 다수 기기 연산 활용 |

| 활용 분야 | 예시 |
|:---------|:-----|
| **모바일** | 키보드 예측, 음성 인식 |
| **헬스케어** | 환자 데이터 분석 |
| **금융** | 이상거래 탐지 |

---

## 연계 토픽

- [Transfer Learning](/docs/ai/07-learning-techniques/transfer-learning)
- [분산 컴퓨팅](/docs/ds/11-distributed/distributed-computing)

---

## 학습 체크리스트

- [ ] 연합학습의 정의와 동작원리 이해
- [ ] 4단계 프로세스(Broadcast-Local Update-Aggregate-Global Update) 암기
- [ ] FedSGD vs FedAVG 차이점 파악

---

## 참고자료

- 정보관리기술사 AI 학습자료
- Google, "Communication-Efficient Learning of Deep Networks from Decentralized Data" (2017)
