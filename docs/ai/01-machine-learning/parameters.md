---
layout: default
title: 경사하강법
parent: 1. 기계학습
grand_parent: AI (인공지능)
nav_order: 15
---

# 경사하강법(Gradient Descent)
{: .fs-8 }

6. 퍼셉트론
{: .label .label-purple }

---

## 핵심 키워드

`학습률` `최적의 파라미터`

---

## 정의/개념

주어진 함수의 최소값 위치를 찾기 위해 비용함수의 가중치 반대 방향으로 정의한 학습률을 가지고 조금씩 움직여 가면서 최적의 파라미터를 찾으려는 기법

---

## 경사하강법의 개념

- 주어진 함수의 최소값 위치를 찾기 위해 비용 함수(Cost Function)의 Gradient 반대 방향으로 정의한 Learning Rate(step size)를 가지고 조금씩 움직여 가면서 최적의 파라미터를 찾으려는 방법
- 머신 러닝 기법 등에서 오차항의 최소값을 찾기 위해 사용되는 기법

---

## 매커니즘

```
    J(w)
      │
      │ Initial      Gradient
      │ weight  ╲  ╱
      │         ─●─
      │          ╲
      │           ╲
      │            ╲        Global cost minimum
      │             ╲          Jmin(w)
      │              ●─────────────────────────
      │
      └──────────────────────────────────────── w
```

### Formal definition

$$cost(W) = \frac{1}{2n}\sum_{i=1}^{n}(Wx^{(i)} - y^{(i)})^2$$

$$W := W - α\frac{∂}{∂W}cost(W)$$

---

## 프로세스

| # | 단계 | 설명 |
|:--|:-----|:-----|
| 1 | 임의의 변수 초기값 선택 | 시작점 설정 |
| 2 | 변수 값에 해당하는 경사도(기울기) 계산 | Gradient 계산 |
| 3 | 변수를 경사방향으로 움직여 다음 기울기 계산 | 이동 및 재계산 |
| 4 | 1),2),3)을 반복하여 함수의 최소값이 되는 값을 찾음 | 반복 수행 |

---

## 주의사항

- 머신 러닝에서 예측 값과 실제 값의 오차를 설명하는 오차함수의 최소값을 찾기 위해 사용
- 적절한 Learning Rate 지정 문제로 인해 오버슈팅 문제를 지니며 이를 해결하기 위한 경사하강법 최적화 알고리즘 존재

---

## 연계 토픽

- [오류역전파](/docs/ai/01-machine-learning/perceptron)
- [옵티마이저](/docs/ai/02-deep-learning/optimizer)

---

## 학습 체크리스트

- [ ] 경사하강법의 정의와 수식 이해
- [ ] 프로세스 4단계 암기
- [ ] Learning Rate와 오버슈팅 문제 이해

---

## 참고자료

- 정보관리기술사 AI 학습자료
